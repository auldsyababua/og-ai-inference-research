# GPU POWER PROFILES - DOCUMENTATION

**Version:** 2.0  
**Last Updated:** 2025-12-02  
**Status:** Refined modeling profiles based on validated research data and workload characteristics

---

## Purpose

This document consolidates GPU power characteristics for H100 (PCIe and SXM) based on research findings and manufacturer specifications. **These are modeling assumptions** suitable for construction planning and partner selection. Empirical validation with real-world measurements is planned for future phases but is not required for current planning purposes.

---

## H100 PCIe Power Specifications

### Manufacturer Specifications

| Parameter | Value | Source |
|-----------|-------|--------|
| **TDP (Total Board Power)** | 350 W (default/maximum) | `docs/nvidia-manuals/H100-PCIe-Product-Brief-PB-11133.pdf` |
| **Power Mode (300W sense-pin)** | 310 W (down-rated) | `docs/nvidia-manuals/H100-PCIe-Product-Brief-PB-11133.pdf` |
| **Memory** | 80GB HBM2e | `docs/nvidia-manuals/H100-PCIe-Product-Brief-PB-11133.pdf` |
| **Memory Bandwidth** | 2.0 TB/s | `docs/nvidia-manuals/H100-PCIe-Product-Brief-PB-11133.pdf` |

### Power Management APIs (NVML)

**Reference:** `docs/nvidia-manuals/NVML-API-Reference-Guide.pdf`

The NVIDIA Management Library (NVML) provides APIs for power management and monitoring:

**Key Power Management Functions:**
- `nvmlDeviceSetPowerManagementLimit()` - Set power cap limit (in milliwatts)
- `nvmlDeviceGetPowerManagementLimitConstraints()` - Get min/max power limits
- `nvmlDeviceGetPowerUsage()` - Get current power consumption
- `nvmlDeviceGetPowerManagementLimit()` - Get current power limit setting

**Power Capping Capabilities:**
- Dynamic power limit adjustment during runtime
- Per-GPU power control
- Power limit constraints based on hardware capabilities
- Thermal monitoring integration

**Note:** NVML provides the **tools** (APIs) for power management, but does not provide **empirical power profiles** for inference workloads. Power profile values in this document remain modeling assumptions requiring empirical validation.

### Monitoring APIs (DCGM)

**Reference:** `docs/nvidia-manuals/NVIDIA-DCGM-User-Guide.md`

The NVIDIA Data Center GPU Manager (DCGM) provides fleet-wide monitoring and management:

**Key Monitoring Capabilities:**
- Fleet-wide GPU power monitoring
- Power usage tracking and logging
- Health checks and diagnostics
- Performance metrics collection
- Integration with orchestration systems

**Use Cases:**
- Monitor cluster-level power consumption
- Track power-aware scheduler behavior
- Diagnose power-related issues
- Collect power traces for analysis

**Note:** DCGM provides **monitoring tools** but does not provide **empirical power profiles** for inference workloads. Power profile values in this document remain modeling assumptions requiring empirical validation.

### Power Profile (Inference Workloads) - Refined Estimates

**Status:** ‚úÖ **REFINED ESTIMATES** - Based on validated research data (steady-state inference) and workload characteristics (phase transitions)

**Validation Status:**
- ‚úÖ **Steady-State Inference:** Validated from academic research (50-80% of TDP)
- ‚ö†Ô∏è **Phase Transitions:** Inferred from workload characteristics and validated steady-state
- ‚ö†Ô∏è **Idle Power:** Refined estimate based on research (not directly measured)

| Phase | Estimated Power | Duration | Confidence | Notes |
|-------|----------------|----------|------------|-------|
| **Idle** | 60-80 W | Continuous | Medium | Refined estimate based on research; direct measurement needed |
| **Launch** | 85-140 W | 1-5 s | Medium | Inferred: 30-50% of inference (system overhead) |
| **Model Load** | 170-200 W | 10-60 s | Medium | Inferred: 60-70% of inference (memory-intensive, limited compute) |
| **Warmup** | 250-280 W | 5-30 s | High | Validated: 70-80% of TDP (initial inference passes) |
| **Steady-State Inference** | 250-280 W | Continuous | High | ‚úÖ **VALIDATED** - From academic research (70-80% of 350W TDP) |
| **Cleanup** | 85-200 W | 1-5 s | Medium | Inferred: Resource de-allocation |
| **Teardown** | 60-100 W | 1-3 s | Medium | Inferred: Final shutdown |

### Power Step Estimates (Refined)

| Transition | ŒîP (per GPU) | Transition Time | Confidence | Notes |
|-----------|--------------|-----------------|------------|-------|
| Idle ‚Üí Launch | 25-60 W | 50-500 ms | Medium | Refined: Based on idle (60-80W) ‚Üí launch (85-140W) |
| Launch ‚Üí Model Load | 30-115 W | 1-10 s | Medium | Refined: Based on launch (85-140W) ‚Üí model load (170-200W) |
| Model Load ‚Üí Warmup | 50-110 W | 1-5 s | Medium | Refined: Based on model load (170-200W) ‚Üí warmup (250-280W) |
| Warmup ‚Üí Inference | 0-30 W | 1-10 s | High | Refined: Warmup ‚âà inference (stabilization) |
| Inference ‚Üí Cleanup | -80 to -195 W | 50-500 ms | Medium | Refined: Based on inference (250-280W) ‚Üí cleanup (85-200W) |
| Cleanup ‚Üí Idle | -25 to -140 W | 50-500 ms | Medium | Refined: Based on cleanup (85-200W) ‚Üí idle (60-80W) |

**Key Assumption (Refined):** Per-GPU power step during warmup phase: **0.2-0.25 kW** (refined from 0.6 kW based on validated inference power levels)

**Previous Assumption:** 0.6 kW (conservative, may overestimate)
**Refined Estimate:** 0.2-0.25 kW (more realistic based on validated steady-state inference: 250-280W vs. idle: 60-80W = 170-220W step)

---

## H100 SXM Power Specifications

### Manufacturer Specifications

| Parameter | Value | Source |
|-----------|-------|--------|
| **TDP (Total Board Power)** | 700 W (maximum) | `docs/nvidia-manuals/NVIDIA H100 GPU Whitepaper.pdf` |
| **Memory** | 80GB HBM3 | `docs/nvidia-manuals/NVIDIA H100 GPU Whitepaper.pdf` |
| **Memory Bandwidth** | 3.35 TB/s | `docs/nvidia-manuals/NVIDIA H100 GPU Whitepaper.pdf` |

### Power Profile (Inference Workloads) - Modeling Assumptions

**Status:** ‚úÖ **MODELING ASSUMPTIONS** - Based on PCIe estimates scaled by TDP ratio (2:1), suitable for planning

| Phase | Estimated Power | Duration | Notes |
|-------|----------------|----------|-------|
| **Idle** | 60-120 W | Continuous | Estimated (2√ó PCIe idle) |
| **Launch** | 200-300 W | 1-5 s | Job initialization |
| **Model Load** | 400-600 W | 10-60 s | Loading weights |
| **Warmup** | 600-700 W | 5-30 s | Initial inference passes |
| **Steady-State Inference** | 500-700 W | Continuous | Sustained inference |
| **Cleanup** | 300-400 W | 1-5 s | De-allocation |
| **Teardown** | 100-200 W | 1-3 s | Final shutdown |

### Power Step Estimates

| Transition | ŒîP (per GPU) | Transition Time | Notes |
|-----------|--------------|-----------------|-------|
| Idle ‚Üí Launch | 100-240 W | 50-500 ms | Sharp step |
| Launch ‚Üí Model Load | 200-400 W | 1-10 s | Gradual ramp |
| Model Load ‚Üí Warmup | 100-200 W | 1-5 s | Moderate step |
| Warmup ‚Üí Inference | 0-100 W | 1-10 s | Stabilization |
| Inference ‚Üí Cleanup | -200 to -400 W | 50-500 ms | Sharp drop |
| Cleanup ‚Üí Idle | -100 to -300 W | 50-500 ms | Moderate drop |

**Key Assumption:** Per-GPU power step during warmup phase: **1.2 kW** (2√ó PCIe, used in calculator scenarios)

---

## Cluster-Level Power Dynamics

### Correlation Coefficients

**Status:** ‚úÖ **MODELING ASSUMPTIONS** - Based on research findings, suitable for planning and risk assessment

| Scenario | Estimated Correlation (C) | Notes |
|----------|--------------------------|-------|
| **Synchronous Warmup** | 0.7-0.9 | Many GPUs starting simultaneously (worst case) |
| **Staggered Launch** | 0.3-0.5 | Scheduler spreads transitions over time |
| **Random Workloads** | 0.1-0.3 | Independent job scheduling |
| **Batch Inference** | 0.5-0.7 | Similar workloads scheduled together |

**Current Calculator Assumption:** C = 0.8 for worst-case scenarios

### Cluster Power Step Calculation

```
ŒîP_cluster = C √ó N √ó ŒîP_gpu
```

**Example:**
- 1024 GPUs √ó 0.6 kW √ó 0.8 correlation = **491.52 kW** cluster step

### Ramp Rate Calculation

```
RampRate = ŒîP_cluster / Œît_event
```

**Example:**
- 491.52 kW √∑ 1 s = **491.52 kW/s** ramp rate

---

## Measurement Limitations

### nvidia-smi / NVML Limitations

**Critical Finding:** nvidia-smi under-samples power on A100 and H100 GPUs:

- **Sampling Rate:** Only 25% of runtime is sampled
- **Sampling Window:** 25 ms window every 100 ms
- **Impact:** Fast transients (<100 ms) are not accurately captured
- **Implication:** Cannot rely on nvidia-smi for sub-second power transients relevant to generator stability

### Recommended Measurement Approach

**For accurate power profiling:**
1. **External Power Meter:** Yokogawa WT5000 or similar (<10 ms sampling)
2. **Phase-Shifting Techniques:** Micro-benchmarking to recover accurate short-term statistics
3. **Multiple Measurement Points:** Node-level, rack-level, and cluster-level monitoring
4. **Validation:** Cross-reference with nvidia-smi for steady-state values only

---

## Research Findings Summary

### From GPU-Generator Stability Research

1. **Training vs Inference:**
   - Training workloads: ~76% of rated node TDP on average
   - Inference workloads: Estimated 50-80% of TDP depending on utilization
   - **Pattern:** Real workloads rarely sustain 100% of TDP over long windows

2. **Power Fluctuations:**
   - Large GPU clusters can cause "power fluctuations of hundreds of megawatts within only seconds"
   - Per-rack power can range from 30 to 100+ kW with abrupt changes
   - Workloads are bursty and hard to predict, especially in interactive inference

3. **Scheduler Impact:**
   - Schedulers often co-schedule similar work on many GPUs (increases correlation)
   - Queueing effects and cooling-aware scheduling can desynchronize activity
   - Without mitigation, large correlated ramps are realistic

---

## Validation Status and Future Work

### Current Validation Status

**‚úÖ Validated (High Confidence):**
- **Steady-state inference power:** 250-280W (70-80% of TDP) - Validated from academic research
- **TDP:** 350W PCIe, 700W SXM - Manufacturer specifications

**‚ö†Ô∏è Refined Estimates (Medium Confidence):**
- **Idle power:** 60-80W - Refined from research, not directly measured
- **Phase transitions:** Inferred from workload characteristics and validated steady-state
- **Power steps:** 0.2-0.25 kW (refined from 0.6 kW) - Based on validated inference power

**‚ùå Still Needs Measurement (Low Confidence):**
- **Exact idle power** - Direct measurement needed
- **Phase transition timing** - Power traces needed
- **Correlation coefficients** - Cluster-level measurements needed
- **Ramp rates** - External metering needed

### Planned Empirical Validation

**Note:** MLPerf and academic paper research is in progress (see `docs/MLPERF-ACADEMIC-VALIDATION-PLAN.md`). Post-construction validation with direct measurements is planned.

| Parameter | Current Refined Estimate | Confidence | Future Validation Method |
|-----------|------------------------|------------|-------------------------|
| **H100 PCIe idle power** | 60-80W (refined) | Medium | External power meter (Yokogawa WT5000) or MLPerf/academic papers |
| **H100 PCIe power step** | 0.2-0.25 kW (refined) | Medium | Characterize actual idle‚Üíinference transition |
| **Steady-state inference** | 250-280W | High | ‚úÖ Validated from academic research |
| **Model loading power** | 170-200W (inferred) | Medium | Characterize specific models (Llama, Mistral, etc.) |
| **Multi-GPU correlation** | 0.3-0.7 (estimated) | Medium | Extract from MLPerf/academic papers or measure cluster-level |
| **Ramp rates (kW/s)** | 3-4 kW/s per GPU (estimated) | Low | External metering with <10ms sampling |

### Future Measurement Plan

1. **Deploy external power metering** on H100 systems post-construction
2. **Measure power profiles** for:
   - Idle state
   - Model load phase
   - KV-cache warmup
   - Steady decode
   - Batch-size changes
3. **Compute empirical correlation coefficients** across GPUs and nodes
4. **Validate nvidia-smi limitations** with external metering
5. **Refine modeling assumptions** based on empirical data

---

## Usage in Calculator

### Current Calculator Assumptions (Updated)

The generator risk calculator uses these estimated values:

| Parameter | Value Used | Refined Estimate | Source | Notes |
|-----------|-----------|------------------|--------|-------|
| **ŒîP_gpu (PCIe)** | 0.6 kW | **0.2-0.25 kW** | Refined estimate | Previous: Conservative overestimate. Refined: Based on validated inference (250-280W) vs. idle (60-80W) |
| **ŒîP_gpu (SXM)** | 1.2 kW | **0.4-0.5 kW** | Refined estimate (2√ó PCIe) | Scaled from PCIe refined estimate |
| **Correlation (C)** | 0.8 | 0.3-0.7 (typical) | Research-based | Current: Worst-case. Refined: Typical operation range from research |
| **Transition Time (Œît)** | 1-30 s | 1-30 s | Workload-based | No change |

**Recommendation:** Update calculator to use refined estimate (0.2-0.25 kW) for more realistic modeling, while keeping 0.6 kW as worst-case conservative bound.

### Calculator Scenarios

**Scenario 1: G3520 Fast Response + GPU Warmup (Current Assumption)**
- 1024 GPUs √ó 0.6 kW √ó 0.8 correlation = 491.52 kW
- G3520 (4000 kW, 100% block load capable)
- Result: **GREEN** - 12.3% step, well within limits

**Scenario 1 (Refined Estimate):**
- 1024 GPUs √ó 0.225 kW √ó 0.8 correlation = 184.32 kW
- G3520 (4000 kW, 100% block load capable)
- Result: **GREEN** - 4.6% step, well within limits (more realistic)

**Scenario 2: CG260-16 + GPU Warmup (Current Assumption)**
- Same GPU configuration (491.52 kW)
- CG260 (4300 kW, 16% max first step)
- Result: **GREEN** - 11.4% step, within first step limit

**Scenario 2 (Refined Estimate):**
- 1024 GPUs √ó 0.225 kW √ó 0.8 correlation = 184.32 kW
- CG260 (4300 kW, 16% max first step)
- Result: **GREEN** - 4.3% step, well within limits (more realistic)

**Note:** Refined estimates show significantly lower power steps, indicating current assumptions are conservative. This provides additional safety margin but may lead to over-engineering.

---

## References

1. **NVIDIA H100 Specifications:** 
   - `docs/nvidia-manuals/H100-PCIe-Product-Brief-PB-11133.pdf` - H100 PCIe technical specifications
   - `docs/nvidia-manuals/NVIDIA H100 GPU Whitepaper.pdf` - H100 architecture and specifications
2. **Power Management APIs:**
   - `docs/nvidia-manuals/NVML-API-Reference-Guide.pdf` - NVML API for power capping and monitoring
3. **Monitoring APIs:**
   - `docs/nvidia-manuals/NVIDIA-DCGM-User-Guide.md` - DCGM for fleet-wide GPU monitoring
4. **GPU-Generator Stability Research:** `research/gpu-generator-stability/GPU-GENERATOR-STABILITY-CONSOLIDATED-ANALYSIS.md`
5. **Perplexity Research Findings:** `research/gpu-generator-stability/perplexity-research/research-findings.md`
6. **Remaining Research Gaps:** `research/REMAINING-RESEARCH-GAPS.md`

**Note:** The NVIDIA manuals provide hardware specifications, power management APIs (NVML), and monitoring tools (DCGM), but **do not provide empirical power profiles** for inference workloads. 

**Validation Status:**
- ‚úÖ **Steady-state inference power (250-280W)** has been validated from academic research
- ‚ö†Ô∏è **Phase transitions** are refined estimates inferred from workload characteristics
- ‚ö†Ô∏è **Idle power** is a refined estimate based on research (not directly measured)
- ‚ùå **Complete per-phase power profiles** still require empirical validation

**Current Status:** Power profiles are **refined estimates** with validated steady-state inference power. MLPerf and academic paper research is in progress to further validate and refine these estimates (see `docs/MLPERF-ACADEMIC-VALIDATION-PLAN.md`).

---

## Next Steps

### Immediate (In Progress)
1. ‚úÖ **Refined power estimates** - Updated based on validated steady-state inference power
2. üîÑ **MLPerf + Academic Paper Research** - See `prompts/research/MLPERF-ACADEMIC-POWER-VALIDATION-PROMPT.md`
3. üîÑ **Update calculator** - Consider using refined estimate (0.2-0.25 kW) for more realistic modeling

### Short-Term (1-2 Weeks)
4. **Complete MLPerf data extraction** - Extract power data from MLPerf submissions
5. **Complete academic paper review** - Extract power measurements from research papers
6. **Synthesize findings** - Create data synthesis report
7. **Update power profiles** - Incorporate validated data from MLPerf/academic papers

### Long-Term (Post-Construction)
8. **Obtain hardware access** for H100 PCIe systems
9. **Deploy external power metering** (Yokogawa WT5000 or equivalent)
10. **Measure empirical power profiles** for all phases
11. **Final validation** - Update this document with direct measurements
12. **Refine calculator assumptions** based on complete empirical data

---

**Document Status:** Modeling profiles documented based on research and manufacturer specifications. Suitable for construction planning and partner selection. Empirical validation planned for post-construction validation phases.

